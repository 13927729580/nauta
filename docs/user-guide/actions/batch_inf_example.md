# Batch Inference Example

For information about stream inference testing, refer to [Stream Inference](streaming_inference.md).

## Example Flow

Following is the general flow of this example:

1. The user has acquired the dataset and the trained model.
1. The user converts dataset into serialized protocol buffers (PBs).
1. The user invokes `dlsctl mount`.
1. The user mounts the Samba shared folder by invoking the command displayed in the previous step.
1. The user copies the serialized PBs and the trained model to the just-mounted share.
1. The user runs `dlsctl predict batch` command.

## MNIST Example

You need to have preprocessed MNIST data for feeding the batch inference. You can use already-preprocessed data 
located in 'parsed' directory, or generate data on your own ('described in MNIST Data Preprocessing below). Perform the following steps:

1. Mount Samba input share to your directory, assumed `/mnt/input` . Use the command printed by 
`dlsctl mount`.
1. Copy 'parsed' and 'trained_mnist_model' to `/mnt/input`.
1. **Execute**: `dlsctl predict batch --model-location /mnt/input/home/trained_mnist_model --data /mnt/input/home/parsed --model-name mnist`

**Notes**:
* Paths provided in locations such as `--model-location` and `--data` need to point for files/directory from the container's context, not from user's filesystem or mounts. These paths can be mapped using instructions from `dlsctl mount`. For
example, if you've mounted Samba `/input` and copied files there, you should pass `/mnt/input/home/<file>` .
* `--model-name` is optional, but it must match the model name provided during data preprocessing, since generated requests
must provide which servable they target. In the `mnist_converter_pb.py` script you can find 
`request.model_spec.name = 'mnist'`, which saves model name in requests, and that name must match value passed as 
`--model-name`. If not provided it assumes that model name is equal to last directory in model location:
`/mnt/input/home/trained_mnist_model` -> `trained_mnist_model`

## MNIST Data Preprocessing
Perform the following steps:
1. Create venv. **Execute**: `make venv`
1. Run the mnist_converter_pb.py script using just generated venv:
```
source .venv/bin/activate
python mnist_converter_pb.py
```

## Description of Files

* output - example PBs responses from batch inference downloaded after `dlsctl predict batch` process.
* parsed - example PBs parsed MNIST dataset generated by `mnist_converter_pb.py` script ready as an input for 
`dlsctl predict batch`.
* trained_mnist_model - example MNIST trained model used for inference.
* batch_client.py - prototype of batch inference wrapper client.
* checker.py - script checking MNIST inference error rate from PBs responses.
* mnist_converter_pb.py - example preprocessing script for preparing predict batch input. handles downloading MNIST 
dataset and saving PBs to 'parsed' folder.
* mnist_input_data.py - dependency for mnist_converter_pb.py script for managing original MNIST dataset.
* requirements.txt - required Python dependencies for invoking all Python scripts here.
* requirements-dev.txt - optional Python dependency for e.g. auto formatting Python code.

## References

* https://www.tensorflow.org/serving/serving_basic
* https://developers.google.com/protocol-buffers/docs/pythontutorial
* https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py
* https://www.tensorflow.org/serving/docker




