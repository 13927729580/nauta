# Troubleshooting

## Jupyter Error 

* __Saving a File Causes the Following Error:__
    
        Creating file failed. An error occurred while creating a new file.
        
        Unexpected error while saving file: input/home/filname [Errno 2]
        No such file or directory: '/mnt/input/home/filename'
        
    The error appears when a user tries to save file in '/input/home' folder, which is a read-only folder. In Jupyter, select the `/output/home` folder to correct this issue.
    
**Note 1:** Closing the Jupyter notebook window in a web browser causes experiments to stop executing. Attaching to the same Jupyter session still shows the stopped experiment.

**Note 2:** This is a known issue in Jupyter [https://github.com/jupyter/notebook/issues/1647](https://github.com/jupyter/notebook/issues/1647). Currently, there is no workaround.
    

## User Management Error 

* __Users with the same name cannot be created directly after its removal__

After deleting a user name and verifying that the user name _is not_ on the list of user names, _it is not_ possible to create a new user with the same name within short period of time. The reason is that the user's related Kubernetes objects are deleted asynchronously by Kubernetes and it can take some time.

**Note:** To resolve, wait a few minutes before creating a user with the same name.


## Docker Error 

* __The Docker installation on the client machine takes up a lot of space and contains a lot of container images__

[https://docs.docker.com](https://docs.docker.com) states:
Docker takes a conservative approach to cleaning up unused objects (often referred to as _garbage collection_), such as images, containers, volumes, and networks: these objects are generally _not_ removed unless you explicitly ask Docker to do so._

**Note:** Refer to the following information for detailed instructions on how to prune unused Docker images: [https://docs.docker.com/config/pruning/](https://docs.docker.com/config/pruning/) 


## DLSCTL Connection Error

* __Launching tensorboard instance and launching WebUI do not work.__
 
After running the `dlsctl launch webui` or the `dlsctl launch tb <experiment_name>` commands, a connection error message may be visible. During the usage of these commands, a proxy tunnel to the cluster is created. As a result, a connection error can be caused by an incorrect user-config generated by Administrator or by incorrect proxy settings in a local machine. To prevent this, be sure that a valid user-config is used and check proxy settings. In addition, ensure that the current proxy settings _do not_ block any connection to a local machine.  
 
 
 ##  Insufficient Resources Causes Experiments Failures 

* __Experiment fails just after submission even if script itself is correct__  

If there _is not_ enough resources on a Kubernetes cluster, the pods used in experiments are evicted. This results in failure of the whole experiment, even if there are no other reasons of this failure like those caused by users (like lack of access to data, errors in scripts and so on).

It is recommended that the Administrator investigate the failures to determine a course of action. For example, why all resources have been consumed and then try to free them. 


## Removal of Docker Images

Due to known errors in Docker Garbage Collector making automatic removal of Docker images is laborious and error-prone.

Before running the Docker Garbage Collector, the administrator should remove images that are no longer needed, using the following procedure:

1) Expose the internal docker registry's API by exposing locally port 5000, exposed by _dls4enterprise-docker-registry_ service located in the _dls4e_ namespace. This can be done, for example by issuing the following command on a machine that has access to Intel DL Studio:
     _kubectl port-forward svc/dls4enterprise-docker-registry 5000 -n dls4e_
     
2) Get a list of images stored in the internal registry by issuing the following command (it is assumed that port 5000 is exposed locally):
     _curl http://localhost:5000/v2/_catalog_  
     
3) From the list of images received in the previous step, choose those that should be removed. For each chosen image, execute the following steps:  
      a) Get the list of tags belonging to the chosen image by issuing the following command:  
     _curl http://localhost:5000/v2/<image_name>/tags/list_  
      
      b) For each tag, get a digest related to this tag:  
     _curl --header "Accept: application/vnd.docker.distribution.manifest.v2+json" http://localhost:5000/v2/<image_name>/manifests/<tag>_  
     Digest is returned in a header of a response under the _Docker-Content-Digest_ key
      
      c) Remove the image from the registry by issuing the following command:  
     _curl -X "DELETE" --header "Accept: application/vnd.docker.distribution.manifest.v2+json" http://localhost:5000/v2/<image_name>/manifests/<digest>_
     
4) Run Docker Garbage Collector by issuing the following command:
     _kubectl exec -it $(kubectl get --no-headers=true pods -l app=docker-registry -n dls4e -o custom-columns=:metadata.name) -n dls4e registry garbage-collect /etc/docker/registry/config.yml_

5) Restart system's Docker registry. It can be done by deleting the pod with label: _dls4e_app_name=docker-registry_ 

## Running Experiments are Evicted After Running Memory-consuming Horovod's Training

When running memory-consuming multi-node, Horovod-based experiments evicts other experiments that work on the same nodes where Horovod-related tasks were scheduled. As a result, such behavior is caused by Horovod-related tasks consume all memory on certain nodes which leads to eviction of other tasks running on these nodes. This is the standard behavior of Kubernetes. 

To prevent such cases, schedule Horovod tasks to be _only one_ on their nodes. It can be achieved by setting the requested number of CPUs when submitting an experiment near to maximum allocatable number of CPUs for cluster's nodes (if cluster has nodes with different numbers, it should be set to the greatest available number of CPUs on nodes. In this case, take into account also number of nodes involved in this experiment).

    * Number of allocatable CPUs can be gathered by executing the kubectl describe node <node_name> command. 
    * Number of allocatable cpus is displayed in the Allocatable->cpu section. 
    * Number of cpus gathered this way should be passed as -p resources.requests.cpu <cpu_number> parameter while issuing experiment submit


## Command Connection Error

After running `dlsctl launch webui` or `dlsctl launch tb <experiment_name>` command connection error message may display. During this command, a proxy tunnel to the cluster is created. Connection errors can be caused by incorrect user config generated by an Administrator or incorrect proxy settings for local machine. 

To prevent this, ensure that a valid user config is used and check proxy settings. In addition, ensure that current proxy settings _do not_ block any connection to a local machine.  


## Inspecting Draft Logs

While viewing the logs and output from some commands, a user may see the following message:
    
    Inspect the logs with `draft logs 01CVMD5D3B42E72CB5YQX1ANS5`
    
However, when running the command, the result is that the command _is not_ found (or the logs _are not_ found if there is a separate draft installation). This is because draft is located in DLSCTL config dir 'config' and needs a `--home` parameter. 

To view the draft logs, use the following command:

    $ ~/config/draft --home ~/config/.draft logs 01CVMD5D3B42E72CB5YQX1ANS5
    
## Experiment's Pods stuck in _Terminating_ state on Node Failure

There may be cases where a node suddenly becomes unavailable, for example due to a power failure. Experiments using TFJob 
templates, which were running on such a node, will stay _Running_ in dlsctl and pods will be _Terminating_ 
indefinitely. An experiment _is not_ rescheduled automatically. 

An example of such occurrence is visible using _kubectl_ tool:
```
user@ubuntu:~$ kubectl get nodes
NAME                                STATUS     ROLES    AGE   VERSION
worker0.node.infra.dlse             NotReady   Worker   7d    v1.10.6
worker1.node.infra.dlse             Ready      Worker   7d    v1.10.6
master.node.infra.dlse              Ready      Master   7d    v1.10.6

user@ubuntu:~$ kubectl get pods
NAME                                   READY   STATUS        RESTARTS   AGE
experiment-master-0                    1/1     Terminating   0          45m
tensorboard-service-5f48964d54-tr7xf   2/2     Terminating   0          49m
tensorboard-service-5f48964d54-wsr6q   2/2     Running       0          43m
tiller-deploy-5c7f4fcb69-vz6gp         1/1     Running       0          49m
```


This is related to unresolved issues found in kubernetes and design of tf-operator:

* https://github.com/kubeflow/tf-operator/issues/720
* https://github.com/kubernetes/kubernetes/issues/55713

To solve this issue, manually resubmit the experiment using dlsctl.

## A multinode, Horovod-based experiment gets FAILED status after pod's failure

If during execution of a multinode, Horovod-based experiment one of pods fails, then the whole experiment gets the FAILED status. Such behaviour is caused by a construction of Horovod framework. This framework uses an Open MPI framework to handle multiple tasks instead of using Kubernetes features. Because of this, it cannot rely also on other Kubernetes features like restarting pods in case of failures. This is why training jobs using Horovod don't resurrect failed pods. The Intel DL Studio system also doesn't restart such training jobs, so if a user encounters such a case - he/she should rerun the experiment manually. The construction of multinode tfjob-based experiments is different as it uses Kubernetes features. Thus, training jobs based on tfjobs restart failing pods so an experiment can be continued after their failure without a need to be restarted manually by a user.

## Nodes Hang When Running Heavy Training Jobs

Running heavy training jobs on workers with the operating system kernel older than 4.* might lead to hanging the worker node. See https://bugzilla.redhat.com/show_bug.cgi?id=1507149

This may occur when a memory limit for 0 job is set to a value close to the maximum amount of memory installed on this node. These problems are caused by errors in handling memory limits in older versions of the kernel. To avoid this problem, it is recommended
to install on all nodes of a cluster with a newer version of a system's kernel.

The following kernel was verified as a viable fix to the issue:


* https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-4.19.2-1.el7.elrepo.x86_64.rpm
      
To install the new kernel please reference chapter 5 "Manually Upgrading the Kernel" in RedHat's Kernel Administration Guide documentation: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/ch-manually_upgrading_the_kernel

**Note:** The above kernel does not include RedHat's optimizations and hardware drivers.

## Platform client for Windows

Using "standard" windows terminals (cmd.exe, power shell) is enough to interact with platform, but there is a sporadic issue with the output. Some lines can be surrounded with incomprehensible characters, for example:
```
[K[?25hCannot connect to K8S cluster: Unable to connect to the server: d...
```
Please note that the recommended shell environment for Windows operating system is bash. For bash-based terminals, this issue does not occur. 
